# AI Analysis Service Configuration

# Google Gemini API Key (required for Gemini analyzer)
# Get this from Google AI Studio (https://ai.google.dev/)
GEMINI_API_KEY="your_gemini_api_key_here"

# Local LLM Model Path (required for Local LLM analyzer)
# Path to your downloaded GGUF-format model compatible with llama-cpp-python
# Examples include Mistral, Llama, or other models in GGUF format
LOCAL_LLM_MODEL_PATH="/path/to/your/downloaded/model.gguf"

# Default AI Analyzer to use (options: 'gemini' or 'local')
DEFAULT_AI_ANALYZER="gemini"

# Optional: GPU Acceleration for Local LLM
# Set to a value greater than 0 to offload model layers to GPU (if available)
LOCAL_LLM_N_GPU_LAYERS=0

# Optional: Context Window Size for Local LLM
# Adjust based on your model's capabilities and available memory
LOCAL_LLM_N_CTX=2048
