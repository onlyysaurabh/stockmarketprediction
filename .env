# MongoDB Configuration
MONGO_URI=mongodb://localhost:27017/
MONGO_DB_NAME=stock_data
FINNHUB_API_KEYS=cvml8uhr01ql90pule5gcvml8uhr01ql90pule60,d010qvhr01qv3oh20410d010qvhr01qv3oh2041g,d010scpr01qv3oh20cd0d010scpr01qv3oh20cdg,d010si1r01qv3oh20dggd010si1r01qv3oh20dh0,d010tphr01qv3oh20kugd010tphr01qv3oh20kv0,d010tu1r01qv3oh20lq0d010tu1r01qv3oh20lqg,d010ug9r01qv3oh20p60d010ug9r01qv3oh20p6g,d010ukhr01qv3oh20q0gd010ukhr01qv3oh20q10,d010ut1r01qv3oh20rpgd010ut1r01qv3oh20rq0,d010v21r01qv3oh20sp0d010v21r01qv3oh20spg,d010v61r01qv3oh20tggd010v61r01qv3oh20th0



# AI Analysis Service Configuration

# Google Gemini API Key (required for Gemini analyzer)
# Get this from Google AI Studio (https://ai.google.dev/)
GEMINI_API_KEY="your_gemini_api_key_here"

# Local LLM Model Path (required for Local LLM analyzer)
# Path to your downloaded GGUF-format model compatible with llama-cpp-python
# Examples include Mistral, Llama, or other models in GGUF format
LOCAL_LLM_MODEL_PATH="/path/to/your/downloaded/model.gguf"

# Default AI Analyzer to use (options: 'gemini' or 'local')
DEFAULT_AI_ANALYZER="gemini"

# Optional: GPU Acceleration for Local LLM
# Set to a value greater than 0 to offload model layers to GPU (if available)
LOCAL_LLM_N_GPU_LAYERS=0

# Optional: Context Window Size for Local LLM
# Adjust based on your model's capabilities and available memory
LOCAL_LLM_N_CTX=2048
